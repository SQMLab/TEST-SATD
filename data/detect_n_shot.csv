id,repository,text,hash,label,code_before,code_after,cot
876542,337,// TODO(b/73762053): Avoid generating unreachable statements.,bd58d2abf2501eec8f23cb94202154d95254c5c4,yes,"@Test
public void testUnreachableCodeGeneration() {
  rewriteGeneratorBody(
      ""if (i) return 1; else return 2;"",
      lines(
          ""  if (i) {"",
          ""    return GEN_CONTEXT$0.return(1);"",
          ""  } else {"",
          ""    return GEN_CONTEXT$0.return(2);"",
          ""  }"",","          // TODO(b/73762053): Avoid generating unreachable statements.
          ""  GEN_CONTEXT$0.jumpToEnd();""));
}

@Test
public void testReturnGenerator() {
  rewriteGeneratorsTest(
      srcs(""function f() { return function *g() {yield 1;} }""),
      expected(
          lines(",This comment includes keyword TODO that indicate Self-admitted technical debt.
176603,56,// Add some role mappings directly in LDAP,3d27ca79dec28538ece8a3669df1fd32b54b1b6e,no,"    UserModel johnRoleMapper = session.users().getUserByUsername(appRealm, ""johnrolemapper"");
    Assert.assertNotNull(johnRoleMapper);
    Assert.assertEquals(0, johnRoleMapper.getRealmRoleMappingsStream().count());

});

testingClient.server().run(session -> {
    LDAPTestContext ctx = LDAPTestContext.init(session);
    RealmModel appRealm = ctx.getRealm();
","// Add some role mappings directly in LDAP
LDAPStorageProvider ldapProvider = LDAPTestUtils.getLdapProvider(session, ctx.getLdapModel());
ComponentModel roleMapperModel = LDAPTestUtils.getSubcomponentByName(appRealm, ctx.getLdapModel(), ""realmRolesMapper"");
RoleLDAPStorageMapper roleMapper = LDAPTestUtils.getRoleMapper(roleMapperModel, ldapProvider, appRealm);

LDAPObject johnLdap = ldapProvider.loadLDAPUserByUsername(appRealm, ""johnrolemapper"");
roleMapper.addRoleMappingInLDAP(""realmRole1"", johnLdap);
roleMapper.addRoleMappingInLDAP(""realmRole2"", johnLdap);

// Get user and check that he has requested roles from LDAP",This comment describes the purpose of code that follows comment and does not indicate any future action is needed.
398677,119,// Alpha does not support DML yet,cf334a26e69e02dc55dae634ec3e31c4a52ebdf8,yes,"    assertUpdate(""CREATE TABLE test_alpha_ddl_partitioned_table (col1 bigint, ds VARCHAR) WITH (format = 'ALPHA', partitioned_by = ARRAY['ds'])"");
    assertUpdate(""ALTER TABLE test_alpha_ddl_partitioned_table ADD COLUMN col2 bigint"");
    assertUpdate(""ALTER TABLE test_alpha_ddl_partitioned_table DROP COLUMN col2"");
    assertUpdate(""DROP TABLE test_alpha_ddl_partitioned_table"");
}

@Test
public void testAlphaFormatDml()
{
    assertUpdate(""CREATE TABLE test_alpha_dml_partitioned_table (col1 bigint, ds VARCHAR) WITH (format = 'ALPHA', partitioned_by = ARRAY['ds'])"");","    // Alpha does not support DML yet
    assertQueryFails(""INSERT INTO test_alpha_dml_partitioned_table VALUES (1, '2022-01-01')"", ""Serializer does not exist: com.facebook.alpha.AlphaSerde"");
    assertUpdate(""DROP TABLE test_alpha_dml_partitioned_table"");
}

@Test
public void testInvokedFunctionNamesLog()
{
    QueryRunner queryRunner = getQueryRunner();
    Session logFunctionNamesEnabledSession = Session.builder(getSession())",Suggest that application does not support DML feature at this point which implicitly suggest in future this feature support need to be added.
909809,374,// check copied position delete row,478ef32a94236f4062458978996539c7c3ffafac,no,"        .rewriteLocationPrefix(table.location(), targetTableLocation())
        .execute();

// We have one more snapshot, an additional manifest list, and a new (delete) manifest,
// and an additional position delete
checkFileNum(4, 3, 3, 13, result);

// copy the metadata files and data files
copyTableFiles(result);
","  // check copied position delete row
  Object[] deletedRow = (Object[]) rows(targetTableLocation() + ""#position_deletes"").get(0)[2];
  assertEquals(
      ""Position deletes should be equal"", new Object[] {1, ""AAAAAAAAAA"", ""AAAA""}, deletedRow);

  // Positional delete affects a single row, so only one row must remain
  assertThat(spark.read().format(""iceberg"").load(targetTableLocation()).count()).isEqualTo(1);
}

@Test",Describes expected behavior of output of test code that follows the comment.
1512588,829,// Probably incorrect - comparing Object[] arrays with Arrays.equals,25f399a80a38c70853f9ee13c3297af9ca4c5126,yes,"
@Override
public boolean equals(Object o) {
    if (this == o)
        return true;
    if (o == null || getClass() != o.getClass())
        return false;

    ComplexBean that = (ComplexBean) o;
","// Probably incorrect - comparing Object[] arrays with Arrays.equals
if (!Arrays.equals(strs, that.strs))
    return false;
// Probably incorrect - comparing Object[] arrays with Arrays.equals
if (!Arrays.equals(jobs, that.jobs))
    return false;
if (list != null ? !list.equals(that.list) : that.list != null)
    return false;
if (map != null ? !map.equals(that.map) : that.map != null)
    return false;","Explicitly indicates the developer's uncertainty about the correctness, implicitly suggesting potential future work."
1197422,567,// Endless loop,cb6986c5b47c9f153c5e82b80fe459cfec66d7a2,no,"
this.topicOperatorMetricsGatherer = new TopicOperatorMetricsCollectionScheduler(this.topicOperatorCollector, ""strimzi.io/cluster="" + this.testStorage.getClusterName());
this.topicOperatorMetricsGatherer.startCollecting();

// we will create incrementally topics
final int batchSize = 100;
final int topicPartitions = 12;
final int topicReplicas = 3;
final int minInSyncReplicas = 2;
","while (true) { // Endless loop
    int start = successfulCreations;
    int end = successfulCreations + batchSize;

    try {
        // Create topics
        KafkaTopicScalabilityUtils.createTopicsViaK8s(testStorage.getNamespaceName(), testStorage.getClusterName(),
            testStorage.getTopicName(), start, end, topicPartitions, topicReplicas, minInSyncReplicas);
        KafkaTopicScalabilityUtils.waitForTopicStatus(testStorage.getNamespaceName(), testStorage.getTopicName(),
            start, end, CustomResourceStatus.Ready, ConditionStatus.True);",Describes code that the comment follows.
203245,60,// THIS FILE IS COPIED FROM APACHE CALCITE,d3b33fef2f8d09456a8486124d5086fa77105cf9,yes,"import java.io.File;
import java.io.IOException;
import java.io.Writer;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
","// THIS FILE IS COPIED FROM APACHE CALCITE

/**
 * A collection of resources used by tests.
 *
 * <p>Loads files containing test input and output into memory. If there are differences, writes out
 * a log file containing the actual output.
 *
 * <p>Typical usage is as follows. A test case class defines a method
 *",It indicates a code duplication issue where a file has been copied rather than properly integrated as a dependency.
1134559,491,// Ignore exceptions,7bca75192d6b9400bbcc2d0814037df389e1cd09,no,"  notificationLog = insertTxnWriteNotificationLog(TXN_ID, WRITE_ID, DB_NAME, TABLE_NAME);
}

@After
public void tearDown() throws Exception {
  try {
    if (client != null) {
      client.close();
    }
  } catch (Exception e) {","  // Ignore exceptions
} finally {
  client = null;
}
try {
  if (notificationLog != null) {
    objectStore.getPersistenceManager().deletePersistent(notificationLog);
  }
} catch (Exception e) {
  // Ignore exceptions",This is bad because the exception is ignored without explanation that may lead to hidden failures so need to handle this.
499770,137,//hacky workaround using the toString method to avoid mocking the Ruby runtime,a50adb8fb3a4179c582da6f34b27eeff12e4fa94,yes,"@RunWith(MockitoJUnitRunner.class)
public class RubyHashGaugeTest {

    @Mock
    RubyHash rubyHash;

    private static final String RUBY_HASH_AS_STRING = ""{}"";

    @Before
    public void _setup() {","    //hacky workaround using the toString method to avoid mocking the Ruby runtime
    when(rubyHash.toString()).thenReturn(RUBY_HASH_AS_STRING);
}

@Test
public void getValue() {
    RubyHashGauge gauge = new RubyHashGauge(""bar"", rubyHash);
    assertThat(gauge.getValue().toString()).isEqualTo(RUBY_HASH_AS_STRING);
    assertThat(gauge.getType()).isEqualTo(MetricType.GAUGE_RUBYHASH);
",Contains the phrase hacky workaround which matches frequently appeared typical keywords indicating SATD.
149266,40,"// Should have 6 total fetches, 3 fetches for topic foo (though 4 partitions but 3 fetches) and 3
// fetches for topic bar (though 3 partitions but 3 fetches).",0a5a56e2ecb01db96e61f3acc949b79386c50359,no,");
expectedMetrics.put(
        metrics.metricName(SharePartitionManager.ShareGroupMetrics.PARTITION_LOAD_TIME_MAX, SharePartitionManager.ShareGroupMetrics.METRICS_GROUP_NAME),
        val -> assertEquals(val, 100.0, SharePartitionManager.ShareGroupMetrics.PARTITION_LOAD_TIME_MAX)
);
expectedMetrics.forEach((metric, test) -> {
    assertTrue(metrics.metrics().containsKey(metric));
    test.accept((Double) metrics.metrics().get(metric).metricValue());
});
","    validateBrokerTopicStatsMetrics(
        brokerTopicStats,
        new TopicMetrics(6, 0, 0, 0),
        Map.of(""foo"", new TopicMetrics(3, 0, 0, 0), ""bar"", new TopicMetrics(3, 0, 0, 0))
    );
}

@Test
public void testMultipleConcurrentShareFetches() throws InterruptedException {
",Describes expected result of test code that follows this comments.
571347,159,"// Theoretically, the HLL algorithm can count very large cardinality with little relative error rate,
// which is less than 2% in Doris. We cost about 5 minutes to test with one billion input numbers here
// successfully, but in order to shorten the test time, we chose one million numbers.",5baba5460e6f4c12a7feb225797a047fd7698dcb,yes,"import org.junit.Assert;
import org.junit.Test;

// hive hll udf test
public class HllUDFTest {

    private BinaryObjectInspector inputOI0 = new JavaConstantBinaryObjectInspector(new byte[0]);

    @Test
    public void hllCardinalityTest() throws Exception {","HllCardinalityUDF hllCardinalityUDF = new HllCardinalityUDF();
hllCardinalityUDF.initialize(new ObjectInspector[] { inputOI0 });
Hll hll = new Hll();
long largeInputSize = 1000000L;
for (long i = 1; i <= largeInputSize; i++) {
    hll.updateWithHash(i);
}
byte[] hllLargeBytes = HllUtil.serializeToBytes(hll);
hllCardinalityUDF.initialize(new ObjectInspector[] { inputOI0 });
Object evaluateLarge = hllCardinalityUDF",It acknowledges a trade-off between accuracy and test efficiency which could impact the reliability of the test.
148864,40,"// After records are acquired, the initialReadGapOffset should be updated",a44b8534af35dd1eeec1e1452294568831483648,no,"assertEquals(SharePartitionState.ACTIVE, sharePartition.partitionState());
assertFalse(sharePartition.cachedState().isEmpty());
assertEquals(11, sharePartition.startOffset());
assertEquals(40, sharePartition.endOffset());
assertEquals(3, sharePartition.stateEpoch());
assertEquals(16, sharePartition.nextFetchOffset());

SharePartition.InitialReadGapOffset initialReadGapOffset = sharePartition.initialReadGapOffset();
assertNotNull(initialReadGapOffset);
","    // After records are acquired, the initialReadGapOffset should be updated
    assertEquals(16, initialReadGapOffset.gapStartOffset());
    assertEquals(40, initialReadGapOffset.endOffset());
}

@Test
public void testAcquireGapAtBeginningAndFetchedRecordsOverlapInFlightBatches() {
    Persister persister = Mockito.mock(Persister.class);
    ReadShareGroupStateResult readShareGroupStateResult = Mockito.mock(ReadShareGroupStateResult.class);
    Mockito.when(readShareGroupStateResult.topicsData()).thenReturn(List.of(",Describes expected result of test code that follows this comments.
1004573,441,"// read() and closeEntry() must throw IOExceptions to indicate that
// the stream is corrupt. The bug above reported that they would loop
// forever.",4ad63a32abf4e1c61ce02cc670432cb488c31dbc,yes,"private static final byte[] INCOMPLETE_ZIP = new byte[] {
        0120, 0113, 0003, 0004, 0024, 0000, 0010, 0010, 0010, 0000, 0002, 0035, (byte) 0330,
        0106, 0000, 0000, 0000, 0000, 0000, 0000, 0000, 0000, 0000, 0000, 0000, 0000, 0013,
        0000, 0000, 0000, 0146, 0157, 0157, 0057, 0142, 0141, 0162, 0056, 0160, 0156, 0147 };

// http://b//21846904
public void testReadOnIncompleteStream() throws Exception {
    ZipInputStream zi = new ZipInputStream(new ByteArrayInputStream(INCOMPLETE_ZIP));
    ZipEntry ze = zi.getNextEntry();
","try {
    zi.read(new byte[1024], 0, 1024);
    fail();
} catch (IOException expected) {
}

try {
    zi.closeEntry();
    fail();
} catch (IOException expected) {",This is a know bug that is reported already and implicitly suggest that it need to be fixed.
776498,269,// make host1 a little worse,9d1264b569f16c4486e10ee9721393f92983e3a9,no,"InetAddressAndPort host2 = InetAddressAndPort.getByName(""127.0.0.3"");
InetAddressAndPort host3 = InetAddressAndPort.getByName(""127.0.0.4"");
InetAddressAndPort host4 = InetAddressAndPort.getByName(""127.0.0.5"");
List<InetAddressAndPort> hosts = Arrays.asList(host1, host2, host3);

// first, make all hosts equal
setScores(dsnitch, 1, hosts, 10, 10, 10);
EndpointsForRange order = full(host1, host2, host3);
Util.assertRCEquals(order, dsnitch.sortedByProximity(self, full(host1, host2, host3)));
","// make host1 a little worse
setScores(dsnitch, 1, hosts, 20, 10, 10);
order = full(host2, host3, host1);
Util.assertRCEquals(order, dsnitch.sortedByProximity(self, full(host1, host2, host3)));

// make host2 as bad as host1
setScores(dsnitch, 2, hosts, 15, 20, 10);
order = full(host3, host1, host2);
Util.assertRCEquals(order, dsnitch.sortedByProximity(self, full(host1, host2, host3)));
",Describes a deliberate modification to the test setup for testing purpose.
